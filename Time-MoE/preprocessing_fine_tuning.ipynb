{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code to prepare the dataset for fine-tuning in jsonl format\n",
    "\n",
    "### Code Description\n",
    "\n",
    "This code prepares the datasets for fine-tuning the TimeMOE model. For each option, a specific dataset will be generated with a different configuration. The output format is JSON Lines (jsonl), which is ideal for training models with large datasets while keeping the data structure lightweight and easy to process.\n",
    "\n",
    "Each dataset configuration can be adjusted according to the task type and defined parameters. The preparation includes organizing the data in a format compatible with model training, such as TimeMOE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_window(series, window):\n",
    "    \"\"\"\n",
    "    Generate rolling window data for time series analysis.\n",
    "\n",
    "    Parameters:\n",
    "    - series: array-like, time series data\n",
    "    - window: int, size of the rolling window\n",
    "\n",
    "    Returns:\n",
    "    - df: pandas DataFrame, containing the rolling window data\n",
    "    - scaler: MinMaxScaler object, used for normalization\n",
    "    \"\"\"\n",
    "    data = []\n",
    "\n",
    "    for i in range(len(series) - window):\n",
    "        example = np.array(series[i:i + window])\n",
    "        data.append(example)\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df = df.dropna()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tuning Global 5 Years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.read_csv('../database/combined_data.csv', sep=\";\")\n",
    "\n",
    "all_data['timestamp'] = pd.to_datetime(all_data['timestamp'], errors='coerce')\n",
    "\n",
    "output_path = 'dataset_global_5_years/'  \n",
    "os.makedirs(output_path, exist_ok=True)  \n",
    "\n",
    "end_date = all_data['timestamp'].max()\n",
    "\n",
    "for years in range(5, 0, -1):\n",
    "    yearly_data = all_data[all_data['timestamp'] <= end_date - pd.DateOffset(years=(5-years))]\n",
    "    state_product_dict = {}\n",
    "\n",
    "    for state in yearly_data['state'].unique():\n",
    "        products = yearly_data[yearly_data['state'] == state]['product'].unique()\n",
    "        state_product_dict[state] = list(products)\n",
    "\n",
    "    output_file = os.path.join(output_path, f'dataset_global_{(end_date - pd.DateOffset(years=(5-years))).year}.jsonl')\n",
    "\n",
    "    with open(output_file, 'w') as file:\n",
    "        for state, products in state_product_dict.items():\n",
    "            for product in products:\n",
    "                data_filtered = yearly_data[(yearly_data['state'] == state) & (yearly_data['product'] == product)]\n",
    "\n",
    "                sequence = data_filtered['m3'][:-12].tolist()\n",
    "\n",
    "                if sequence: \n",
    "                    json_line = {\n",
    "                        f'sequence': sequence\n",
    "                    }\n",
    "                    file.write(json.dumps(json_line) + '\\n')\n",
    "\n",
    "print(\"Data has been successfully saved by year.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tuning Global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered data has been saved to dataset_global/dataset_global.jsonl\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Uncomment and comment out the code below as needed.\n",
    "\n",
    "# Load the combined dataset\n",
    "all_data = pd.read_csv('../database/combined_data.csv', sep=\";\")\n",
    "\n",
    "state_product_dict = {}\n",
    "\n",
    "for state in all_data['state'].unique():\n",
    "    products = all_data[all_data['state'] == state]['product'].unique()\n",
    "    state_product_dict[state] = list(products)\n",
    "\n",
    "output_file = 'dataset_global/dataset_global.jsonl'\n",
    "\n",
    "# Open the output file in write mode\n",
    "with open(output_file, 'w') as file:\n",
    "    \n",
    "    ''' \n",
    "    # INFO: ======== Raw Data ========\n",
    "    ''' \n",
    "    for state, products in state_product_dict.items():\n",
    "        for product in products:\n",
    "\n",
    "            # Filter data for the current state and product\n",
    "            data_filtered = all_data[(all_data['state'] == state) & (all_data['product'] == product)]\n",
    "\n",
    "            sequence = data_filtered['m3'][:-12].tolist()\n",
    "            json_line = {'sequence': sequence}\n",
    "\n",
    "            file.write(json.dumps(json_line) + '\\n')\n",
    "    \n",
    "    ''' \n",
    "    # INFO: ======== Noramlizados ========\n",
    "    ''' \n",
    "    # for state, products in state_product_dict.items():\n",
    "    #     for product in products:\n",
    "    #         data_filtered = all_data[(all_data['state'] == state) & (all_data['product'] == product)]\n",
    "            \n",
    "    #         data = rolling_window(data_filtered['m3'][:-12], 12)\n",
    "    #         print(data)\n",
    "\n",
    "    #         sequence = data.values  \n",
    "\n",
    "    #         print(sequence)\n",
    "            \n",
    "    #         scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    #         sequence_scaled = scaler.fit_transform(sequence.reshape(-1, 1)).flatten()\n",
    "    #         print(sequence_scaled)\n",
    "            \n",
    "    #         json_line = {\"sequence\": sequence_scaled.tolist()} \n",
    "            \n",
    "    #         file.write(json.dumps(json_line) + '\\n')\n",
    "    \n",
    "    ''' \n",
    "    # INFO: ======== Rolling Window ========\n",
    "    ''' \n",
    "    # for state, products in state_product_dict.items():\n",
    "    #     for product in products:\n",
    "    #         data_filtered = all_data[(all_data['state'] == state) & (all_data['product'] == product)]\n",
    "            \n",
    "    #         m3_values = data_filtered['m3'][:-12].values\n",
    "            \n",
    "    #         scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    #         m3_scaled = scaler.fit_transform(m3_values.reshape(-1, 1)).flatten()\n",
    "\n",
    "    #         data = rolling_window(m3_scaled, 12)\n",
    "            \n",
    "    #         for _, row in data.iterrows():\n",
    "    #             print(row.values.tolist())\n",
    "    #             json_line = {\"sequence\": row.values.tolist()}  \n",
    "    #             file.write(json.dumps(json_line) + '\\n')\n",
    "    \n",
    "\n",
    "print(f\"Filtered data has been saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tuning Product 5 Years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been successfully saved by product and year.\n"
     ]
    }
   ],
   "source": [
    "database_path = '../database/combined_data.csv'\n",
    "all_data = pd.read_csv(database_path, sep=';')\n",
    "all_data['timestamp'] = pd.to_datetime(all_data['timestamp'], errors='coerce')\n",
    "\n",
    "output_path = 'dataset_product_5_years/'  \n",
    "os.makedirs(output_path, exist_ok=True)  \n",
    "\n",
    "end_date = all_data['timestamp'].max()\n",
    "\n",
    "for years in range(5, 0, -1):\n",
    "    yearly_data = all_data[all_data['timestamp'] <= end_date - pd.DateOffset(years=(5 - years))]\n",
    "    \n",
    "    for product in yearly_data['product'].unique():\n",
    "        product_data = yearly_data[yearly_data['product'] == product]\n",
    "        state_product_dict = {}\n",
    "\n",
    "        for state in product_data['state'].unique():\n",
    "            state_product_dict[state] = product\n",
    "        \n",
    "        # Criar nome do arquivo com o produto\n",
    "        product_clean = product.replace(\" \", \"_\").replace(\"/\", \"_\").replace(\"\\\\\", \"_\")  # Evitar problemas com nomes de arquivos\n",
    "        output_file = os.path.join(output_path, f'dataset_product_{product_clean}_{(end_date - pd.DateOffset(years=(5 - years))).year}.jsonl')\n",
    "\n",
    "        with open(output_file, 'w') as file:\n",
    "            for state in product_data['state'].unique():\n",
    "                data_filtered = product_data[product_data['state'] == state]\n",
    "                \n",
    "                sequence = data_filtered['m3'][:-12].tolist()\n",
    "                \n",
    "                if sequence:\n",
    "                    json_line = {\n",
    "                        'sequence': sequence\n",
    "                    }\n",
    "                    file.write(json.dumps(json_line) + '\\n')\n",
    "\n",
    "print(\"Data has been successfully saved by product and year.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tuning Product "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been successfully saved by product and year.\n"
     ]
    }
   ],
   "source": [
    "database_path = '../database/combined_data.csv'\n",
    "all_data = pd.read_csv(database_path, sep=';')\n",
    "all_data['timestamp'] = pd.to_datetime(all_data['timestamp'], errors='coerce')\n",
    "\n",
    "output_path = 'dataset_product/'  \n",
    "os.makedirs(output_path, exist_ok=True)  \n",
    "\n",
    "for product in all_data['product'].unique():\n",
    "    product_data = all_data[all_data['product'] == product]\n",
    "    state_product_dict = {}\n",
    "\n",
    "    for state in product_data['state'].unique():\n",
    "        state_product_dict[state] = product\n",
    "    \n",
    "    # Criar nome do arquivo com o produto\n",
    "    product_clean = product.replace(\" \", \"_\").replace(\"/\", \"_\").replace(\"\\\\\", \"_\")  # Evitar problemas com nomes de arquivos\n",
    "    output_file = os.path.join(output_path, f'dataset_product_{product_clean}.jsonl')\n",
    "\n",
    "    with open(output_file, 'w') as file:\n",
    "        for state in product_data['state'].unique():\n",
    "            data_filtered = product_data[product_data['state'] == state]\n",
    "            \n",
    "            sequence = data_filtered['m3'][:-12].tolist()\n",
    "            \n",
    "            if sequence:\n",
    "                json_line = {\n",
    "                    'sequence': sequence\n",
    "                }\n",
    "                file.write(json.dumps(json_line) + '\\n')\n",
    "\n",
    "print(\"Data has been successfully saved by product and year.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tuning Indiv 5 Years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.read_csv('../database/combined_data.csv', sep=\";\")\n",
    "\n",
    "all_data['timestamp'] = pd.to_datetime(all_data['timestamp'], errors='coerce')\n",
    "\n",
    "output_base_path = 'dataset_individual_5_anos/'  \n",
    "os.makedirs(output_base_path, exist_ok=True)  \n",
    "\n",
    "end_date = all_data['timestamp'].max()\n",
    "\n",
    "for years in range(5, 0, -1):\n",
    "    yearly_data = all_data[all_data['timestamp'] <= end_date - pd.DateOffset(years=(5-years))]\n",
    "\n",
    "    state_product_dict = {}\n",
    "\n",
    "    for state in yearly_data['state'].unique():\n",
    "        products = yearly_data[yearly_data['state'] == state]['product'].unique()\n",
    "        state_product_dict[state] = list(products)\n",
    "\n",
    "    for state, products in state_product_dict.items():\n",
    "        for product in products:\n",
    "            data_filtered = yearly_data[(yearly_data['state'] == state) & (yearly_data['product'] == product)]\n",
    "\n",
    "            sequence = data_filtered['m3'][:-12].tolist()\n",
    "\n",
    "            if sequence: \n",
    "                json_line = {\n",
    "                    'sequence': sequence\n",
    "                }\n",
    "\n",
    "                year = (end_date - pd.DateOffset(years=(5-years))).year\n",
    "                output_path = os.path.join(output_base_path, f'dataset_individual_{year}')\n",
    "                os.makedirs(output_path, exist_ok=True) \n",
    "\n",
    "                output_file = os.path.join(output_path, f'dataset_{state}_{product}.jsonl')\n",
    "\n",
    "                with open(output_file, 'w') as file:\n",
    "                    file.write(json.dumps(json_line) + '\\n')\n",
    "\n",
    "print(f\"Individual files have been successfully saved in {output_base_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tuning Indiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual files have been successfully\n"
     ]
    }
   ],
   "source": [
    "all_data = pd.read_csv('../database/combined_data.csv', sep=\";\")\n",
    "\n",
    "all_data['timestamp'] = pd.to_datetime(all_data['timestamp'], errors='coerce')\n",
    "\n",
    "output_base_path = 'dataset_individual/'  \n",
    "os.makedirs(output_base_path, exist_ok=True)  \n",
    "\n",
    "state_product_dict = {}\n",
    "\n",
    "for state in all_data['state'].unique():\n",
    "    products = all_data[all_data['state'] == state]['product'].unique()\n",
    "    state_product_dict[state] = list(products)\n",
    "\n",
    "for state, products in state_product_dict.items():\n",
    "    for product in products:\n",
    "        data_filtered = all_data[(all_data['state'] == state) & (all_data['product'] == product)]\n",
    "\n",
    "        sequence = data_filtered['m3'][:-12].tolist()\n",
    "\n",
    "        if sequence: \n",
    "            json_line = {\n",
    "                'sequence': sequence\n",
    "            }\n",
    "\n",
    "            output_file = os.path.join(output_base_path, f'dataset_{state}_{product}.jsonl')\n",
    "\n",
    "            with open(output_file, 'w') as file:\n",
    "                file.write(json.dumps(json_line) + '\\n')\n",
    "\n",
    "print(f\"Individual files have been successfully\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-24.06",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
